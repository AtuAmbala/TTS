{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preprocessing\n",
    "\n",
    "Load and Clean Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('issue_data_sna.csv')\n",
    "\n",
    "# Parse dates and calculate issue lifetime in hours\n",
    "data['Open Date'] = pd.to_datetime(data['Open Date'])\n",
    "data['Closed Date'] = pd.to_datetime(data['Closed Date'])\n",
    "data['Issue Lifetime (hours)'] = (data['Closed Date'] - data['Open Date']).dt.total_seconds() / 3600\n",
    "\n",
    "# Bin issue lifetimes\n",
    "bins = [0, 48, 336, float('inf')]  # <2 days, 2 days < x < 2 weeks, >2 weeks\n",
    "labels = ['< 2 days', '2 days < x < 2 weeks', '> 2 weeks']\n",
    "data['Issue Lifetime'] = pd.cut(data['Issue Lifetime (hours)'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    text = text.replace('\"', '')  # Remove double quotes\n",
    "    text = re.sub(r'DevTools.*?\\(automated\\)', '', text)  # Remove specific text\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = emoji.demojize(text)  # Remove emojis\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)  # Remove punctuation\n",
    "    text = text.replace(\"#\", \"\").replace(\"\\n\", \"\").replace(\"\\r\", \"\")  # Remove unwanted characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    words = text.split()\n",
    "    words = [word for word in words if len(word) <= 20]  # Remove long words\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Clean the text columns\n",
    "data['Issue Title'] = data['Issue Title'].apply(clean_text)\n",
    "data['Issue Body'] = data['Issue Body'].apply(clean_text)\n",
    "data['Comment Bodies'] = data['Comment Bodies'].apply(lambda x: clean_text(str(x)))\n",
    "data['Comment Authors'] = data['Comment Authors'].apply(lambda x: str(x))\n",
    "\n",
    "# Merge text columns for processing\n",
    "data['Issue Text'] = data['Issue Title'] + ' ' + data['Issue Body'] + ' ' + data['Comment Bodies']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature Engineering\n",
    "Compute Social Network Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get social network metrics\n",
    "def get_social_network_metrics(comment_body_list, comment_author_list):\n",
    "    if len(comment_author_list) <= 1:\n",
    "        return 0, 0, 0, 0, 0, 0, 1 if len(comment_author_list) == 1 else 0\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(comment_author_list) - 1):\n",
    "        G.add_edge(comment_author_list[i + 1], comment_author_list[i])\n",
    "\n",
    "    if G.number_of_edges() == 0:\n",
    "        return 0, 0, 0, 0, 0, 0, len(set(comment_author_list))\n",
    "\n",
    "    wordiness = sum(len(s) for s in comment_body_list) // len(comment_body_list) if comment_body_list else 0\n",
    "    centrality = nx.degree_centrality(G)\n",
    "    average_degree_centrality = sum(centrality.values()) / len(centrality) if centrality else 0\n",
    "    closeness = nx.closeness_centrality(G)\n",
    "    average_closeness = sum(closeness.values()) / len(closeness) if closeness else 0\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    average_betweenness = sum(betweenness.values()) / len(betweenness) if betweenness else 0\n",
    "    density = nx.density(G)\n",
    "    edges = nx.number_of_edges(G)\n",
    "    num_discussants = len(set(comment_author_list))\n",
    "    \n",
    "    return wordiness, average_degree_centrality, average_closeness, average_betweenness, density, edges, num_discussants\n",
    "\n",
    "# Apply social network metrics to each row\n",
    "data[['wordiness', 'average_degree_centrality', 'average_closeness', \n",
    "      'average_betweenness', 'density', 'edges', 'num_discussants']] = data.apply(\n",
    "    lambda row: get_social_network_metrics(row['Comment Bodies'].split('|SEPARATOR|'), row['Comment Authors'].split('|SEPARATOR|')), axis=1, result_type='expand'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Model Training  \n",
    "Prepare Data and Fine-Tune GPT Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training File ID: file-hu9XpjMVo5onPj18LGGIQMUp\n",
      "{\n",
      "  \"id\": \"ftjob-9O0pKBiwri1NFq79I5xAs0QQ\",\n",
      "  \"created_at\": 1717747828,\n",
      "  \"error\": {\n",
      "    \"code\": null,\n",
      "    \"message\": null,\n",
      "    \"param\": null\n",
      "  },\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"finished_at\": null,\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": \"auto\",\n",
      "    \"batch_size\": \"auto\",\n",
      "    \"learning_rate_multiplier\": \"auto\"\n",
      "  },\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"organization_id\": \"org-iXy8zWZJPm0q9LU8ZICnDE47\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"validating_files\",\n",
      "  \"trained_tokens\": null,\n",
      "  \"training_file\": \"file-hu9XpjMVo5onPj18LGGIQMUp\",\n",
      "  \"validation_file\": null,\n",
      "  \"user_provided_suffix\": null,\n",
      "  \"seed\": 1324902560,\n",
      "  \"estimated_finish\": null,\n",
      "  \"integrations\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "# Read the OpenAI API key from a file\n",
    "with open('openAiKey.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Function to prepare data for GPT fine-tuning\n",
    "def prepare_data_for_gpt(df):\n",
    "    training_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        prompt = (\n",
    "            f\"Predict the time to solve this GitHub issue based on its content:\\n\\n\"\n",
    "            f\"Title: {row['Issue Title']}\\n\"\n",
    "            f\"Body: {row['Issue Body']}\\n\"\n",
    "            f\"Labels: {row['Labels']}\\n\"\n",
    "            f\"Number of Comments: {row['Number of Comments']}\\n\"\n",
    "            f\"Comment Bodies: {row['Comment Bodies']}\\n\"\n",
    "            f\"Social Metrics: wordiness={row['wordiness']}, average_degree_centrality={row['average_degree_centrality']}, \"\n",
    "            f\"average_closeness={row['average_closeness']}, average_betweenness={row['average_betweenness']}, \"\n",
    "            f\"density={row['density']}, edges={row['edges']}, num_discussants={row['num_discussants']}\\n\"\n",
    "            f\"\\nResponse:\"\n",
    "        )\n",
    "        completion = row['Issue Lifetime']\n",
    "        training_data.append({'prompt': prompt, 'completion': f\" {completion}\"})\n",
    "    return training_data\n",
    "\n",
    "# Prepare the data\n",
    "training_data = prepare_data_for_gpt(data)\n",
    "\n",
    "# Save as JSONL file\n",
    "with open('training_data.jsonl', 'w') as f:\n",
    "    for entry in training_data:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "# Upload the training file\n",
    "upload_response = client.files.create(\n",
    "    file=open('training_data.jsonl', 'rb'),\n",
    "    purpose='fine-tune'\n",
    ")\n",
    "\n",
    "training_file_id = upload_response.id\n",
    "print(f\"Training File ID: {training_file_id}\")\n",
    "\n",
    "# Create a fine-tune job\n",
    "fine_tune_response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print(fine_tune_response.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Monitoring the Fine-Tuning Process\n",
    "To monitor the fine-tuning job, we can periodically check its status until it completes. Once the job is complete, we can retrieve details about the fine-tuned model.\n",
    "\n",
    "Monitoring and Retrieving the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def validate_jsonl_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line_number, line in enumerate(f, start=1):\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                if 'prompt' not in record or 'completion' not in record:\n",
    "                    print(f\"Line {line_number}: Missing 'prompt' or 'completion' field.\")\n",
    "                if not isinstance(record['prompt'], str) or not isinstance(record['completion'], str):\n",
    "                    print(f\"Line {line_number}: 'prompt' and 'completion' should be strings.\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Line {line_number}: JSONDecodeError - {e}\")\n",
    "\n",
    "# Validate the training data file\n",
    "validate_jsonl_file('training_data.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Job ID: ftjob-9O0pKBiwri1NFq79I5xAs0QQ\n",
      "Initial Status: failed\n",
      "Fine-tuning failed. Please check the details and logs for more information.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Function to check the status of the fine-tuning job\n",
    "def check_fine_tune_status(job_id):\n",
    "    status_response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    return status_response\n",
    "\n",
    "# Fine-tuning job ID\n",
    "fine_tune_job_id = fine_tune_response.id\n",
    "print(f\"Fine-tuning Job ID: {fine_tune_job_id}\")\n",
    "\n",
    "# Monitor the fine-tuning job status\n",
    "status = check_fine_tune_status(fine_tune_job_id)\n",
    "print(f\"Initial Status: {status.status}\")\n",
    "\n",
    "# Poll the status until the job is complete\n",
    "while status.status not in ['succeeded', 'failed']:\n",
    "    time.sleep(60)  # Wait for 60 seconds before checking the status again\n",
    "    status = check_fine_tune_status(fine_tune_job_id)\n",
    "    print(f\"Current Status: {status.status}\")\n",
    "\n",
    "# Check the final status and retrieve model details if successful\n",
    "if status.status == 'succeeded':\n",
    "    fine_tuned_model = status.fine_tuned_model\n",
    "    print(f\"Fine-tuning succeeded. Fine-tuned Model: {fine_tuned_model}\")\n",
    "else:\n",
    "    print(\"Fine-tuning failed. Please check the details and logs for more information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717747840: The job failed due to an invalid training file. Invalid file format. Input file file-hu9XpjMVo5onPj18LGGIQMUp is in the prompt-completion format, but the specified model gpt-3.5-turbo-0125 is a chat model and requires chat-formatted data. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for details.\n",
      "1717747828: Validating training file: file-hu9XpjMVo5onPj18LGGIQMUp\n",
      "1717747828: Created fine-tuning job: ftjob-9O0pKBiwri1NFq79I5xAs0QQ\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve fine-tuning job logs\n",
    "def get_fine_tune_logs(job_id):\n",
    "    logs_response = client.fine_tuning.jobs.list_events(job_id)\n",
    "    return logs_response\n",
    "\n",
    "# Retrieve the logs for the failed fine-tuning job\n",
    "logs = get_fine_tune_logs(fine_tune_job_id)\n",
    "\n",
    "# Print the logs\n",
    "for log in logs.data:\n",
    "    print(f\"{log.created_at}: {log.message}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are an assistant that helps predict the time to resolve GitHub issues.\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Title: enhancement request for macos add text that says \\u201c\\u2318 enter to submit lesson\\nBody: is your feature request related to a problem please describesummary on macos platform it\\u2019s not made clear that commandenter can be used to submit lessons despite the fact that this keyboard shortcut does work and is preferable i suggest making this clear to macbook the lessons say check your code ctrl enter on mac the ctrl key is in a slightly awkward position compared to the command key the command key is very easy and quick to hitthis might seem like a small request but i think that the change would be greatly appreciated and helpful to those who are a on macs and b appreciate keyboard shortcuts my personal story is i suffered through the lessons for awhile trying to get accustomed to ctrl enter but it was always a little bother once i figured out the trick of getting to the next page via \\u2318 enter i thought i wish i had known about that earlierstaff requested i open this as an issue describe the solution youd likethis could possibly be implemented in one of two ways change the text on macos to \\u201ccheck your code command enter\\u201d or perhaps \\u201ccheck your code \\u2318 enter pop up on macos that says \\u201cyou can also use commandenter to check your code\\u201d on the popup also include a checkbox \\u201cdon\\u2019t show this again\\u201d describe alternatives youve considerednone additional contextno response\\nLabels: help wanted, other: device specific, scope: UI, type: feature request\\nNumber of Comments: 3\\nComment Bodies: ill add my two cents here i think we should remove the extra hint text completely and the button should just read check your code this information can be made available in the keyboard shortcuts helper dialog that we already have availableseparatori disagree bruce in order for people to find this modal they would need to enable it in the settings first then go through a couple of technical tricks to get it to show up which is hard for a casual user to comprehendthis change would be for the better i will put a help wanted label on itseparatorhi everyone ive just submitted this pr 54276 which should enhance labelling on checkcode buttonmy solution at the moment provides the following label check your code \\u2318 enter which was the fastest way to improve the current situation ive also seen you talked about showing popup we could think about it but i think that updating the label should be better in terms of maintaining and testing stufflet me know what you think about itseparator\\nSocial Metrics: wordiness=987.0, average_degree_centrality=0.6666666666666666, average_closeness=0.38888888888888884, average_betweenness=0.16666666666666666, density=0.3333333333333333, edges=2.0, num_discussants=3.0\\n\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"2 days < x < 2 weeks\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are an assistant that helps predict the time to resolve GitHub issues.\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Title: typo on step 61 of sudoku solver project in scientific computing with python beta cert\\nBody: typo on step 61 of sudoku solver project in scientific computing with python beta cert withing in the hint text when im sure you meant within img width595 altscreen shot 20240321 at 1 39 54 pm srcthanks for all the content\\nLabels: type: bug, scope: curriculum, first timers only, new python course\\nNumber of Comments: 1\\nComment Bodies: thanks for opening this issuethis looks something that can be fixed by first time code contributors to this repository here are the files that you should be looking at to work on a fixlist of files1 please make sure you read our guidelines for contributing we prioritize contributors following the instructions in our guides join us in our chat room or the forum if you need help contributing our moderators will guide you through thissometimes we may get more than one pull requests we typically accept the most quality contribution followed by the one that is made firsthappy\\nSocial Metrics: wordiness=0.0, average_degree_centrality=0.0, average_closeness=0.0, average_betweenness=0.0, density=0.0, edges=0.0, num_discussants=1.0\\n\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"< 2 days\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to prepare data for chat-based fine-tuning\n",
    "def prepare_chat_data(df):\n",
    "    chat_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that helps predict the time to resolve GitHub issues.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Title: {row['Issue Title']}\\nBody: {row['Issue Body']}\\nLabels: {row['Labels']}\\nNumber of Comments: {row['Number of Comments']}\\nComment Bodies: {row['Comment Bodies']}\\nSocial Metrics: wordiness={row['wordiness']}, average_degree_centrality={row['average_degree_centrality']}, average_closeness={row['average_closeness']}, average_betweenness={row['average_betweenness']}, density={row['density']}, edges={row['edges']}, num_discussants={row['num_discussants']}\\n\"},\n",
    "            {\"role\": \"assistant\", \"content\": row['Issue Lifetime']}\n",
    "        ]\n",
    "        chat_data.append({\"messages\": messages})\n",
    "    return chat_data\n",
    "\n",
    "# Prepare the chat data\n",
    "chat_data = prepare_chat_data(data)\n",
    "\n",
    "# Save as JSONL file\n",
    "with open('chat_training_data.jsonl', 'w') as f:\n",
    "    for entry in chat_data:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "# Print sample of the chat data\n",
    "print(json.dumps(chat_data[:2], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the Chat-Formatted Training File and Create a New Fine-Tune Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training File ID: file-mgVn8vMoZkEUnxypkwArQq7b\n",
      "FineTuningJob(id='ftjob-7KYfEjLJRIIzjxM0sgnaxp3g', created_at=1717748347, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-iXy8zWZJPm0q9LU8ZICnDE47', result_files=[], status='validating_files', trained_tokens=None, training_file='file-mgVn8vMoZkEUnxypkwArQq7b', validation_file=None, user_provided_suffix=None, seed=624297924, estimated_finish=None, integrations=[])\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Read the OpenAI API key from a file\n",
    "with open('openAiKey.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Upload the chat-formatted training file\n",
    "upload_response = client.files.create(\n",
    "    file=open('chat_training_data.jsonl', 'rb'),\n",
    "    purpose='fine-tune'\n",
    ")\n",
    "\n",
    "training_file_id = upload_response.id\n",
    "print(f\"Training File ID: {training_file_id}\")\n",
    "\n",
    "# Create a new fine-tune job\n",
    "fine_tune_response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print(fine_tune_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Job ID: ftjob-7KYfEjLJRIIzjxM0sgnaxp3g\n",
      "Initial Status: validating_files\n",
      "Current Status: validating_files\n",
      "Current Status: validating_files\n",
      "Current Status: validating_files\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: running\n",
      "Current Status: succeeded\n",
      "Fine-tuning succeeded. Fine-tuned Model: ft:gpt-3.5-turbo-0125:personal::9XQB3irt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Function to check the status of the fine-tuning job\n",
    "def check_fine_tune_status(job_id):\n",
    "    status_response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    return status_response\n",
    "\n",
    "# Fine-tuning job ID\n",
    "fine_tune_job_id = fine_tune_response.id\n",
    "print(f\"Fine-tuning Job ID: {fine_tune_job_id}\")\n",
    "\n",
    "# Monitor the fine-tuning job status\n",
    "status = check_fine_tune_status(fine_tune_job_id)\n",
    "print(f\"Initial Status: {status.status}\")\n",
    "\n",
    "# Poll the status until the job is complete\n",
    "while status.status not in ['succeeded', 'failed']:\n",
    "    time.sleep(60)  # Wait for 60 seconds before checking the status again\n",
    "    status = check_fine_tune_status(fine_tune_job_id)\n",
    "    print(f\"Current Status: {status.status}\")\n",
    "\n",
    "# Check the final status and retrieve model details if successful\n",
    "if status.status == 'succeeded':\n",
    "    fine_tuned_model = status.fine_tuned_model\n",
    "    print(f\"Fine-tuning succeeded. Fine-tuned Model: {fine_tuned_model}\")\n",
    "else:\n",
    "    print(\"Fine-tuning failed. Please check the details and logs for more information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'data' is your original DataFrame\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are an assistant that helps predict the time to resolve GitHub issues.\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Title: challenege instructions does not match challenge code\\nBody: challenge target even numbered elements using jquery has an issueuser agent is mozilla50 windows nt 100 win64 x64 rv520 gecko20100101 firefox520please describe how to reproduce this issue and include links to screenshots if possiblethe challenge instructions state note that jquery is zeroindexed meaning that counterintuitively odd selects the second element fourth element and so on try selecting all the evennumbered elements and giving them the classes of animated and shakehowever the checks sayyou should use the even function to modify these elements which means that the odd numbered elements assuming that the previous explanation is accurate would be changed not the even either the instruction text should be updated to read try selecting all the oddnumbered elements or the code needs tweaked so that odd would be the correct code instead of evenmy codehtml target1csscolor red target1propdisabled true target4remove red orange green bounce shake jquery playground leftwell target1 target2 target3 rightwell target4 target5 target6\\nLabels: nan\\nNumber of Comments: 1\\nComment Bodies: i agree that this challenge is a bit confusing it talks about even then odd and then even againill make a pr to update instructions see if i can make them a little clearerseparator\\nSocial Metrics: wordiness=0.0, average_degree_centrality=0.0, average_closeness=0.0, average_betweenness=0.0, density=0.0, edges=0.0, num_discussants=1.0\\n\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"> 2 weeks\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are an assistant that helps predict the time to resolve GitHub issues.\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Title: remove replit and replitnix files from boilerplate repos\\nBody: one of the replit devs mentioned that theyre making a lot of changes on their end and recommends that we remove the replit and replitnix files from our boilerplate repos whenever possible we should be able to do this in most of the boilerplate repos without issuethere may be a couple of exceptions though like with the rust in replit project boilerplate for that project there are some commands that need to run when the project is importedthe recommendation for repos that need special commands is to leave the replit and replitnix files in the repo but to make sure the only difference between them and the autogenerated replit and replitnix files are the commands that need to runheres a list of the project boilerplates that should be updated please let me know if i missed any remove replit and replitnix files back end development and apis practice projects x managing packages with npm boilerplate repo prs boilerplate learn description x basic node and express boilerplate repo prs boilerplate learn description x mongodb and mongoose boilerplate repo prs boilerplate learn description required projects x timestamp microservice boilerplate repo prs boilerplate learn description x request header parser microservice boilerplate repo prs boilerplate learn description x url shortener microservice boilerplate repo prs boilerplate learn description x exercise tracker boilerplate repo prs boilerplate learn description x file metadata microservice boilerplate repo prs boilerplate learn description quality assurance practice projects x quality assurance and testing with chai boilerplate repo prs boilerplate learn description x advanced node and express boilerplate repo prs boilerplate learn description required projects x metricimperial converter boilerplate repo prs boilerplate learn description x issue tracker boilerplate repo prs boilerplate learn description x personal library boilerplate repo prs boilerplate learn description x sudoku solver boilerplate repo prs boilerplate learn description x american british translator boilerplate repo prs boilerplate learn description scientific computing with python required projects x arithmetic formatter boilerplate repo prs boilerplate learn description x time calculator boilerplate repo prs boilerplate learn description x budget app boilerplate repo prs boilerplate learn description x polygon area calculator boilerplate repo prs boilerplate learn description x probability calculator boilerplate repo prs boilerplate learn description data analysis with python required projects x meanvariancestandard deviation calculator boilerplate repo prs boilerplate learn description x demographic data analyzer boilerplate repo prs boilerplate learn description x medical data visualizer boilerplate repo prs boilerplate learn description x page view time series visualizer boilerplate repo prs boilerplate learn description x sea level predictor boilerplate repo prs boilerplate learn description information security practice projects x information security with helmetjs boilerplate repo prs boilerplate learn description required projects x stock price checker boilerplate repo prs boilerplate learn description x anonymous message board boilerplate repo prs boilerplate learn description x port scanner boilerplate repo prs boilerplate learn description x sha1 password cracker boilerplate repo prs boilerplate learn description x secure real time multiplayer game boilerplate repo prs boilerplate learn description machine learning with python required projects x rock paper scissors boilerplate repo prs boilerplate learn description special cases x rust in replit tasks import the project into replit copy the autogenerated replit and replitnix files back into the boilerplate add the special commands back to the boilerplates replit file \\u2014 the only diff between replits autogenerated replit file and the one in the boilerplate are those special commands boilerplate repo prs boilerplate learn description na\\nLabels: help wanted, status: discussing, scope: curriculum, status: PR in works\\nNumber of Comments: 9\\nComment Bodies: excellent issue writeup as always kris rocket i will tackle the rust in replit projectseparatorhey scissorsneedfoodtoo we should be fine to open this for contribution yesseparatorthanks for opening up that pr shaunshamilton ill take a look at it nownaomilgbt yes it should be fine to open this up for contribution now at least for the changes to the boilerplate repos i did a bit of testing with the different project types and didnt run into any issues after removing the replit and replitnix filesthe only real difference i noticed is that while replit seems to pick up a run command for node projects from the packagejson file its slightly different for python projectswhen importing a node project all learns have to do is click the done button in the replit windowwhen importing a python project learners have to click use run command and the done button the command python3 mainpy is already prefilledthat config menu defaults to use default interpreter with a dropdown learners could select the mainpy file from the dropdown then click the done button but the other method seems a bit easier to communicatewell probably want to add simple instructions on how to set that run command for both node and python projects in the descriptions on learn then once those descriptions are live we can merge the changes to the boilerplate reposseparatorhi team to solve this monotonous task in a fun and efficient way i created a script for automating this full processheres the link to that project i am not sure what exactly should i add to the learn description so with any pointers on that and ill get that done as well separatorhow far off is fcc from being able to have the users create these project on fcc controlled servers instead of constantly changing the repos because replit does not have a clue of the mess they create every time they make a config changeseparator hi team to solve this monotonous task in a fun and efficient way i created a script for automating this full process heres the link to that project i am not sure what exactly should i add to the learn description so with any pointers on that and ill get that done as well thank you for sharing your script princemendiratta that looks super helpful i was wondering how you were able to update so many of the boilerplate repos simultaneouslyfor the learn descriptions i think a large pr that updates all the english challenge project files is the best way to do this that will help in terms of orchestrating the reviews updating the client and merging all of the boilerplate prswe would definitely welcome your help with updating the learn files if youre interestedbut before we do that we can discuss the language to use firsttheres a slight difference between importing a nodejs and a python boilerplate into replit still itd be good if we could use the same language for both types of projectsmost of the challenge project descriptions contain this text clone this github repo and complete your project locally use our replit starter project to complete your project use a site builder of your choice to complete the project be sure to incorporate all the files from our github repoand the python projects contain this textyou will be working on this project with our replit starter codethe easiest thing to do is to add a couple of short sentences to each description just after use our repit starter project and you will be working on something like after importing this project into replit you will see the replit window select use run command and click the done buttonit would be best to put backticks around replit use run command and done so those dont get translatedso the full sentences could read use our replit starter project to complete your project after importing this project into replit you will see the replit window select use run command and click the done button you will be working on this project with our replit starter code after importing this project into replit you will see the replit window select use run command and click the done buttonor if theres a better more concise way to explain those steps we can go with that when can we work on merging these prs as well and closing this issue for goodseparatorhey princemendiratta thanks for your patience and again for all of your help with these updateswe plan to do a new release of learn soon once the updated text you added in goes live we can go in and merge all of the prs you opened in the boilerplate repos listed abovethis is something we need to coordinate internally but i will definitely tag you and close this issue once thats all done slightlysmilingface separatorthank you again for all of your help updating these boilerplate repos and descriptions princemendirattajust went through and merged all of the prs listed here also cloned and tested a few projects and everything seems to work as expectedit should be safe to close this now and ill do more thorough testing after the weekendseparator\\nSocial Metrics: wordiness=4964.0, average_degree_centrality=0.7, average_closeness=0.3883333333333333, average_betweenness=0.18333333333333332, density=0.35, edges=7.0, num_discussants=5.0\\n\"\n",
      "      },\n",
      "      {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"> 2 weeks\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def prepare_chat_data(df):\n",
    "    chat_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that helps predict the time to resolve GitHub issues.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Title: {row['Issue Title']}\\nBody: {row['Issue Body']}\\nLabels: {row['Labels']}\\nNumber of Comments: {row['Number of Comments']}\\nComment Bodies: {row['Comment Bodies']}\\nSocial Metrics: wordiness={row['wordiness']}, average_degree_centrality={row['average_degree_centrality']}, average_closeness={row['average_closeness']}, average_betweenness={row['average_betweenness']}, density={row['density']}, edges={row['edges']}, num_discussants={row['num_discussants']}\\n\"},\n",
    "            {\"role\": \"assistant\", \"content\": row['Issue Lifetime']}\n",
    "        ]\n",
    "        chat_data.append({\"messages\": messages})\n",
    "    return chat_data\n",
    "\n",
    "# Prepare the chat data\n",
    "chat_train_data = prepare_chat_data(train_data)\n",
    "\n",
    "# Save as JSONL file\n",
    "with open('chat_training_data.jsonl', 'w') as f:\n",
    "    for entry in chat_train_data:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "# Print sample of the chat data\n",
    "print(json.dumps(chat_train_data[:2], indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Status: succeeded\n",
      "Fine-tuning succeeded. Fine-tuned Model: ft:gpt-3.5-turbo-0125:personal::9XQB3irt\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "# Read the OpenAI API key from a file\n",
    "with open('openAiKey.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Fine-tuning job ID (replace with your actual fine-tuning job ID)\n",
    "fine_tune_job_id = \"ftjob-7KYfEjLJRIIzjxM0sgnaxp3g\"\n",
    "\n",
    "# Function to check the status of the fine-tuning job and retrieve the model ID\n",
    "def check_fine_tune_status(job_id):\n",
    "    status_response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    return status_response\n",
    "\n",
    "# Monitor the fine-tuning job status\n",
    "status = check_fine_tune_status(fine_tune_job_id)\n",
    "print(f\"Initial Status: {status.status}\")\n",
    "\n",
    "# Poll the status until the job is complete\n",
    "while status.status not in ['succeeded', 'failed']:\n",
    "    time.sleep(60)  # Wait for 60 seconds before checking the status again\n",
    "    status = check_fine_tune_status(fine_tune_job_id)\n",
    "    print(f\"Current Status: {status.status}\")\n",
    "\n",
    "# Check the final status and retrieve model details if successful\n",
    "if status.status == 'succeeded':\n",
    "    fine_tuned_model = status.fine_tuned_model\n",
    "    print(f\"Fine-tuning succeeded. Fine-tuned Model: {fine_tuned_model}\")\n",
    "else:\n",
    "    print(\"Fine-tuning failed. Please check the details and logs for more information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "data.to_csv('output.csv', index=False)\n",
    "test_data.to_csv('test_output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
