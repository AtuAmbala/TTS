{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('output.csv')\n",
    "test_data= pd.read_csv('test_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Status: succeeded\n",
      "Fine-tuning succeeded. Fine-tuned Model: ft:gpt-3.5-turbo-0125:personal::9XQB3irt\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "# Read the OpenAI API key from a file\n",
    "with open('openAiKey.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Fine-tuning job ID (replace with your actual fine-tuning job ID)\n",
    "fine_tune_job_id = \"ftjob-7KYfEjLJRIIzjxM0sgnaxp3g\"\n",
    "\n",
    "# Function to check the status of the fine-tuning job and retrieve the model ID\n",
    "def check_fine_tune_status(job_id):\n",
    "    status_response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    return status_response\n",
    "\n",
    "# Monitor the fine-tuning job status\n",
    "status = check_fine_tune_status(fine_tune_job_id)\n",
    "print(f\"Initial Status: {status.status}\")\n",
    "\n",
    "# Poll the status until the job is complete\n",
    "while status.status not in ['succeeded', 'failed']:\n",
    "    time.sleep(60)  # Wait for 60 seconds before checking the status again\n",
    "    status = check_fine_tune_status(fine_tune_job_id)\n",
    "    print(f\"Current Status: {status.status}\")\n",
    "\n",
    "# Check the final status and retrieve model details if successful\n",
    "if status.status == 'succeeded':\n",
    "    fine_tuned_model = status.fine_tuned_model\n",
    "    print(f\"Fine-tuning succeeded. Fine-tuned Model: {fine_tuned_model}\")\n",
    "else:\n",
    "    print(\"Fine-tuning failed. Please check the details and logs for more information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Issue Title  Issue Lifetime  \\\n",
      "0                                sync footer with news             NaN   \n",
      "1               codepen signup link leads to 404 error             NaN   \n",
      "2    react write a react component from scratch tes...             NaN   \n",
      "3                           overflow hidden in firefox             NaN   \n",
      "4                            refactor menu to use refs             NaN   \n",
      "..                                                 ...             ...   \n",
      "593  fix tests for strict order in step 25 of oop p...             NaN   \n",
      "594  missleading comments in js exercise declare a ...             NaN   \n",
      "595                                post userreportuser             NaN   \n",
      "596                  update learn footer based on news             NaN   \n",
      "597     browser frozen on fibonacci lost weeks of work             NaN   \n",
      "\n",
      "    Predicted Issue Lifetime  \n",
      "0                   < 2 days  \n",
      "1       2 days < x < 2 weeks  \n",
      "2                  > 2 weeks  \n",
      "3       2 days < x < 2 weeks  \n",
      "4       2 days < x < 2 weeks  \n",
      "..                       ...  \n",
      "593     2 days < x < 2 weeks  \n",
      "594                > 2 weeks  \n",
      "595     2 days < x < 2 weeks  \n",
      "596                > 2 weeks  \n",
      "597                > 2 weeks  \n",
      "\n",
      "[598 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Read the OpenAI API key from a file\n",
    "with open('openAiKey.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Example usage of the fine-tuned chat model\n",
    "def predict_issue_resolution_time(issue_text, model):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that helps predict the time to resolve GitHub issues.\"},\n",
    "            {\"role\": \"user\", \"content\": issue_text}\n",
    "        ],\n",
    "        max_tokens=10  # Adjust as needed\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Prepare the test issues for prediction\n",
    "test_issues = test_data.apply(lambda row: f\"Title: {row['Issue Title']}\\nBody: {row['Issue Body']}\\nLabels: {row['Labels']}\\nNumber of Comments: {row['Number of Comments']}\\nComment Bodies: {row['Comment Bodies']}\\nSocial Metrics: wordiness={row['wordiness']}, average_degree_centrality={row['average_degree_centrality']}, average_closeness={row['average_closeness']}, average_betweenness={row['average_betweenness']}, density={row['density']}, edges={row['edges']}, num_discussants={row['num_discussants']}\\n\", axis=1)\n",
    "\n",
    "# Use the correct fine-tuned model ID\n",
    "fine_tuned_model = \"ft:gpt-3.5-turbo-0125:personal::9XQB3irt\"  # Replace with your actual fine-tuned model ID\n",
    "\n",
    "# Predict and validate on the test data\n",
    "test_data['Predicted Issue Lifetime'] = test_issues.apply(lambda issue_text: predict_issue_resolution_time(issue_text, fine_tuned_model))\n",
    "\n",
    "# Print the predictions\n",
    "print(test_data[['Issue Title', 'Issue Lifetime', 'Predicted Issue Lifetime']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize issue lifetime numerically\n",
    "def categorize_lifetime(hours):\n",
    "    try:\n",
    "        hours = float(hours)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    if hours < 48:\n",
    "        return 1  # <2 days\n",
    "    elif hours < 336:  # 2 weeks = 14 days * 24 hours = 336 hours\n",
    "        return 2  # 2 days < x < 2 weeks\n",
    "    else:\n",
    "        return 3  # >2 weeks\n",
    "\n",
    "# Apply the categorization to actual and predicted lifetimes\n",
    "test_data['Issue Lifetime Bin'] = test_data['Issue Lifetime (hours)'].apply(categorize_lifetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize issue lifetime from words\n",
    "def categorize_lifetime_words(bin):\n",
    "    if bin ==\"<2 days\":\n",
    "        return 1  # <2 days\n",
    "    elif bin ==\"2 days < x < 2 weeks\":  # 2 weeks = 14 days * 24 hours = 336 hours\n",
    "        return 2  # 2 days < x < 2 weeks\n",
    "    else:\n",
    "        return 3  # >2 weeks\n",
    "    \n",
    "\n",
    "# Apply the categorization to actual and predicted lifetimes\n",
    "test_data['Predicted Issue Lifetime Bin'] = test_data['Predicted Issue Lifetime'].apply(categorize_lifetime_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Issue Lifetime Bin Distribution:\n",
      "Issue Lifetime Bin\n",
      "3    252\n",
      "2    182\n",
      "1    164\n",
      "Name: count, dtype: int64\n",
      "Predicted Issue Lifetime Bin Distribution:\n",
      "Predicted Issue Lifetime Bin\n",
      "3    377\n",
      "2    221\n",
      "Name: count, dtype: int64\n",
      "Sample Actual and Predicted Issue Lifetime Bins:\n",
      "   Issue Lifetime Bin  Predicted Issue Lifetime Bin\n",
      "0                   1                             3\n",
      "1                   1                             2\n",
      "2                   2                             3\n",
      "3                   1                             2\n",
      "4                   2                             2\n",
      "5                   1                             2\n",
      "6                   3                             3\n",
      "7                   1                             2\n",
      "8                   2                             2\n",
      "9                   3                             2\n",
      "Categorization Function Test:\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where categorization failed (None values)\n",
    "test_data = test_data.dropna(subset=['Issue Lifetime Bin', 'Predicted Issue Lifetime Bin'])\n",
    "\n",
    "# Check Data Distribution\n",
    "print(\"Actual Issue Lifetime Bin Distribution:\")\n",
    "print(test_data['Issue Lifetime Bin'].value_counts())\n",
    "\n",
    "print(\"Predicted Issue Lifetime Bin Distribution:\")\n",
    "print(test_data['Predicted Issue Lifetime Bin'].value_counts())\n",
    "\n",
    "# Display some of the actual and predicted values\n",
    "print(\"Sample Actual and Predicted Issue Lifetime Bins:\")\n",
    "print(test_data[['Issue Lifetime Bin', 'Predicted Issue Lifetime Bin']].head(10))\n",
    "\n",
    "# Test the categorization function with known values\n",
    "print(\"Categorization Function Test:\")\n",
    "print(categorize_lifetime(24))  # Expected output: 1 (<2 days)\n",
    "print(categorize_lifetime(100))  # Expected output: 2 (2 days < x < 2 weeks)\n",
    "print(categorize_lifetime(500))  # Expected output: 3 (>2 weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "# Calculate weighted average F1-score, precision, and recall\n",
    "labels = [1, 2, 3]\n",
    "f1 = f1_score(test_data['Issue Lifetime Bin'], test_data['Predicted Issue Lifetime Bin'], labels=labels, average='weighted')\n",
    "precision = precision_score(test_data['Issue Lifetime Bin'], test_data['Predicted Issue Lifetime Bin'], labels=labels, average='weighted')\n",
    "recall = recall_score(test_data['Issue Lifetime Bin'], test_data['Predicted Issue Lifetime Bin'], labels=labels, average='weighted')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(test_data['Issue Lifetime Bin'], test_data['Predicted Issue Lifetime Bin'], labels=labels)\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: {'TP': 0, 'FP': 0, 'FN': 164, 'TN': 434}\n",
      "2: {'TP': 88, 'FP': 133, 'FN': 94, 'TN': 283}\n",
      "3: {'TP': 172, 'FP': 205, 'FN': 80, 'TN': 141}\n"
     ]
    }
   ],
   "source": [
    "# Calculate TP, FP, FN, TN\n",
    "results = {}\n",
    "for i, label in enumerate(labels):\n",
    "    results[label] = {'TP': cm[i, i]}\n",
    "    results[label]['FP'] = cm[:, i].sum() - cm[i, i]\n",
    "    results[label]['FN'] = cm[i, :].sum() - cm[i, i]\n",
    "    results[label]['TN'] = cm.sum() - (results[label]['TP'] + results[label]['FP'] + results[label]['FN'])\n",
    "\n",
    "# Print results\n",
    "for label, metrics in results.items():\n",
    "    print(f\"{label}: {metrics}\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df['F1-score'] = f1\n",
    "results_df['Recall'] = recall\n",
    "results_df['Precision'] = precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             <2 days       0.00      0.00      0.00       164\n",
      "2 days < x < 2 weeks       0.40      0.48      0.44       182\n",
      "            >2 weeks       0.46      0.68      0.55       252\n",
      "\n",
      "            accuracy                           0.43       598\n",
      "           macro avg       0.28      0.39      0.33       598\n",
      "        weighted avg       0.31      0.43      0.36       598\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/atu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/atu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "print(classification_report(test_data['Issue Lifetime Bin'], test_data['Predicted Issue Lifetime Bin'], target_names=['<2 days', '2 days < x < 2 weeks', '>2 weeks']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
